---
layout: default
title: Expectation
nav_order: 5
parent: Probability & Statistics
---

## Expectativa

El concepto de **expectativa** (o valor esperado) es fundamental en el campo de la probabilidad y estad칤sticas. **La expectativa proporciona un resumen num칠rico central de una variable aleatoria, ofreciendo una medida de la "tendencia central" o el promedio a largo plazo que uno puede esperar en una serie de experimentos** donde la variable aleatoria se observa repetidamente.

{: .highlight}
La expectativa de una variable aleatoria $$X$$, denotada como $$E[X]$$ o $$\mu$$, es el promedio ponderado de todos los posibles valores que $$X$$ puede tomar, ponderados por la probabilidad de cada valor. Matem치ticamente se define de diferentes maneras dependiendo de si la variable aleatoria es discreta o continua:

- **Variable Discreta**: Si $$X$$ es una variable aleatoria discreta que toma valores $$(x_1, x_2, x_3, \dots)$$ con probabilidades $$(p_1, p_2, p_3, \dots)$$, entonces el valor esperado de $$X$$ se calcula como:

  $$
  E[X] = \sum_{i} x_i p_i
  $$

  donde la suma se extiende sobre todos los valores posibles de $$X$$.

Ejemplo: Calcularemos la expectativa de una variable aleatoria que sigue una distribuci칩n binomial, donde 
$$洧녵 = 10$$ (n칰mero de ensayos) y $$洧녷 = 0.5$$ (probabilidad de 칠xito en cada ensayo).

```python
import scipy.stats as stats

# Par치metros de la distribuci칩n binomial
n = 10  # n칰mero de ensayos
p = 0.5  # probabilidad de 칠xito

# Crear una variable aleatoria binomial
binomial_var = stats.binom(n, p)

# Calcular la expectativa
expected_value = binomial_var.mean()
print("Expectativa de la variable binomial:", expected_value)
```

- **Variable Continua**: Si $$X$$ es una variable aleatoria continua con una funci칩n de densidad de probabilidad (PDF) $$f(x)$$, el valor esperado se define como:

  $$
  E[X] = \int_{-\infty}^{\infty} x f(x) \, dx
  $$

  donde la integral se eval칰a sobre todo el rango de $$X$$.

Ejemplo: Calcular la expectativa de una variable aleatoria que sigue una distribuci칩n normal, con media $$洧랞 = 0$$ y desviaci칩n est치ndar $$洧랥 = 1$$.

```python
# Par치metros de la distribuci칩n normal
mu = 0  # media
sigma = 1  # desviaci칩n est치ndar

# Crear una variable aleatoria normal
normal_var = stats.norm(mu, sigma)

# Calcular la expectativa
expected_value = normal_var.mean()
print("Expectativa de la variable normal:", expected_value)
```

### Generalizando el C치lculo de la Expectativa

Para otras distribuciones, puedes seguir un procedimiento similar utilizando las funciones proporcionadas por SciPy para esa distribuci칩n espec칤fica. Aqu칤 est치 c칩mo se har칤a de forma general:

```python
# Para una distribuci칩n general, reemplazar 'dist' con el nombre de la distribuci칩n
# y configurar sus par치metros espec칤ficos.
dist_var = stats.dist(param1, param2, ...)

# Calcular la expectativa
expected_value = dist_var.mean()
print("Expectativa de la distribuci칩n:", expected_value)
```

## Ley de los Grandes Numeros

{: .note}
La Ley de los Grandes N칰meros (LLN) es un teorema fundamental en teor칤a de la probabilidad que describe el resultado de realizar el mismo experimento un gran n칰mero de veces. Seg칰n la LLN, **el promedio de los resultados obtenidos de un gran n칰mero de pruebas se acercar치 al valor esperado a medida que se incremente el n칰mero de pruebas**. Esto implica que las frecuencias relativas de los resultados convergen a las probabilidades te칩ricas.

### LLN D칠bil vs. LLN Fuerte

- **LLN D칠bil**: Asegura que, para cualquier tolerancia $$\epsilon > 0$$, la probabilidad de que el promedio de las muestras difiera del valor esperado por m치s de $$\epsilon$$ tiende a cero conforme el tama침o de la muestra se hace infinitamente grande.

- **LLN Fuerte**: Establece que con probabilidad 1, el promedio de las muestras converge al valor esperado cuando el tama침o de la muestra tiende a infinito.

### Simulaciones Estad칤sticas en Python

#### Generaci칩n de Datos

Usaremos Python para simular datos utilizando m칩dulos como `numpy` y `scipy`. Por ejemplo, para simular una variable aleatoria binomial, podemos usar el siguiente c칩digo:

```python
import numpy as np
# Simular 1000 ensayos de una distribuci칩n binomial con n=10 y p=0.5
data = np.random.binomial(10, 0.5, 1000)
```

#### C치lculo de Promedios

Para calcular el promedio de los datos simulados y compararlos con el valor esperado:

```python
average = np.mean(data)
print("Promedio calculado:", average)
```

#### An치lisis de Datos y Estimaci칩n de Par치metros

- Interpretaci칩n de Resultados: Los resultados de las simulaciones deben interpretarse en el contexto de la LLN. Por ejemplo, si el promedio calculado de los ensayos binomiales difiere significativamente del valor esperado (5 en este caso), se deber칤a aumentar el n칰mero de ensayos.

- Estimaci칩n de Par치metros: La estimaci칩n de par치metros puede mejorar utilizando m칠todos como el de M치xima Verosimilitud o los M칠todos de Momentos. Esto permite ajustar los par치metros de la distribuci칩n a partir de los datos observados para que coincidan m치s estrechamente con el valor te칩rico esperado.

```python
import matplotlib.pyplot as plt
import numpy as np

# Simular progresivamente mayores cantidades de ensayos
sample_sizes = np.arange(100, 10000, 10)
averages = [np.mean(np.random.binomial(10, 0.5, size)) for size in sample_sizes]

# Graficar los resultados
plt.plot(sample_sizes, averages, label="Promedio de Ensayos Binomiales")
plt.axhline(y=5, color='r', linestyle='--', label="Valor Esperado")
plt.xlabel("Tama침o de la Muestra")
plt.ylabel("Promedio Calculado")
plt.legend()
plt.show()
```

![Grandes Numeros](https://fer78docs.github.io/assets/images/grandes_numeros.png)


## Independencia y Distribuci칩n Id칠ntica de las Muestras (IID)

{: .note}
Datos independientes e id칠nticamente distribuidos (IID) **son aquellos donde cada dato en una muestra es generado de manera independiente de los otros y todos siguen exactamente la misma distribuci칩n de probabilidad.** Esta propiedad es esencial para aplicar correctamente la Ley de los Grandes N칰meros.

En contextos experimentales, garantizar que los datos son IID implica dise침ar experimentos donde las intervenciones externas o las condiciones previas no afectan los resultados de cada prueba. Por ejemplo, en ensayos cl칤nicos, los sujetos deben ser asignados aleatoriamente a grupos de tratamiento para asegurar la independencia.

### Relaci칩n entre Muestras y la Poblaci칩n

- **Media de la Poblaci칩n vs. Media de la Muestra**: La media de la poblaci칩n es el valor promedio de un par치metro en toda la poblaci칩n, mientras que la media de la muestra se calcula de la misma manera pero solo utilizando una parte de la poblaci칩n. La Ley de los Grandes N칰meros asegura que estas dos medias converjan a medida que el tama침o de la muestra aumenta.

- **Estimaci칩n de Par치metros**: La estimaci칩n de par치metros se realiza utilizando la media de la muestra y otras estad칤sticas para hacer inferencias sobre la poblaci칩n total. Esto es fundamental en 치reas como la demograf칤a y la investigaci칩n de mercado, donde no es pr치ctico estudiar a toda la poblaci칩n.

### Ejemplos de Variables Aleatorias y sus Expectativas

**Variables Discretas Comunes**:
   - **Bernoulli**: Eventos de 칠xito/fallo; la expectativa es $$p$$.
   - **Binomial**: N칰mero de 칠xitos en $$n$$ ensayos de Bernoulli; expectativa $$np$$.
   - **Geom칠trica**: N칰mero de ensayos hasta el primer 칠xito; expectativa $$1/p$$.
   - **Poisson**: N칰mero de eventos en un intervalo fijo; expectativa $$\lambda$$.

**Variables Continuas Comunes**:
   - **Normal (Gaussiana)**: Distribuida sim칠tricamente alrededor de la media; la expectativa es $$\mu$$.
   - **Exponencial**: Tiempo entre eventos en un proceso de Poisson; expectativa $$1/\lambda$$.

## Momentos 

{: .highlight}
Los **momentos** son medidas que **capturan caracter칤sticas importantes sobre la forma de una distribuci칩n de probabilidad. Estos incluyen la tendencia central, la dispersi칩n y la forma de la distribuci칩n.** Los momentos se definen generalmente respecto al origen o respecto a la media de la distribuci칩n, y cada uno de ellos ofrece informaci칩n espec칤fica sobre la distribuci칩n.

### Momentos Respecto al Origen

Los momentos respecto al origen se calculan como el valor esperado de las potencias sucesivas de la variable aleatoria $$X$$. Matem치ticamente, el $$k$$-칠simo momento respecto al origen de una variable aleatoria $$X$$ se define como:

$$\mu'_k = E[X^k]$$

donde $$E[\cdot]$$ denota el valor esperado y $$k$$ es un entero no negativo.

### Momentos Centrales

Los momentos centrales, por otro lado, se calculan respecto a la media de la distribuci칩n y proporcionan informaci칩n sobre la variabilidad y la forma de la distribuci칩n m치s all치 de la media. El $$k$$-칠simo momento central se define como:

$$\mu_k = E[(X - E[X])^k]$$

### Tipos de Momentos y su Significado

1. **Primer Momento (Media):**
   - El primer momento respecto al origen es simplemente la media de la distribuci칩n, $$E[X]$$.
   - Este momento mide la tendencia central de la distribuci칩n.

2. **Segundo Momento (Varianza):**
   - El segundo momento central, conocido como la varianza, $$\text{Var}(X) = E[(X - E[X])^2]$$, mide la dispersi칩n de la distribuci칩n alrededor de su media.
   - Cuanto mayor es la varianza, m치s dispersos est치n los datos alrededor de la media.

3. **Tercer Momento (Asimetr칤a):**
   - El tercer momento central est치 relacionado con la asimetr칤a (skewness) de la distribuci칩n.
   - Una asimetr칤a positiva indica una distribuci칩n con una cola m치s larga hacia la derecha; una negativa indica una cola m치s larga hacia la izquierda.

4. **Cuarto Momento (Curtosis):**
   - El cuarto momento central se relaciona con la curtosis, que mide la "altitud" y la "anchura" de las colas de la distribuci칩n.
   - La curtosis compara la cantidad de datos en las colas con la cantidad de datos cerca de la media, proporcionando una percepci칩n de la forma general de la distribuci칩n.

{: .highlight}
Los momentos son fundamentales en estad칤stica porque permiten describir y comparar distribuciones de manera cuantitativa. Son esenciales en muchas aplicaciones estad칤sticas, incluyendo el ajuste de modelos, pruebas de hip칩tesis y en el desarrollo de teoremas centrales como el Teorema del L칤mite Central. Adem치s, los momentos son cruciales para entender propiedades de distribuciones te칩ricas como la normal, la binomial, y otras distribuciones comunes en estudios estad칤sticos.

## Varianza 

### Transformaci칩n de Variables Aleatorias

{: .highlight}
Una variable aleatoria transformada surge cuando se aplica una funci칩n a una variable aleatoria. Ejemplo de transformaci칩n: $$Y = 3X^2 + 9$$, donde $$X$$ es la variable aleatoria original y $$Y$$ es la nueva variable.

#### Varianza y sus Implicaciones

1. **Definici칩n y C치lculo**:
   - La varianza $$\text{Var}(X)$$ es el valor esperado de la desviaci칩n cuadrada desde la media: $$\text{Var}(X) = E[(X - E[X])^2]$$.
   - Para una distribuci칩n normal, la varianza se denota como \( \sigma^2 \).

2. **Interpretaci칩n**:
   - Una alta varianza indica una amplia dispersi칩n de los puntos de datos alrededor de la media, sugiriendo valores de datos diversos.
   - En el contexto de las distribuciones normales, entender la varianza es crucial para comprender la dispersi칩n general de la distribuci칩n.

3. **Relevancia Estad칤stica**:
   - A diferencia de la media, la ley de los grandes n칰meros no se aplica a la varianza, lo que significa que las varianzas de muestras pueden no converger a la varianza de la poblaci칩n sin suposiciones o correcciones adicionales.
