---
layout: default
title: Discrete Distributions
nav_order: 6
parent: Probability & Statistics
---

## Temas

 - [Distribucion Binomial](#distribucion-binomial)

Los tipos de variables aleatorias discretas se clasifican com√∫nmente seg√∫n las distribuciones de probabilidad que describen c√≥mo se comportan los datos asociados a estas variables. 

## Distribucion Binomial

{: .note}
La distribuci√≥n binomial o distribuci√≥n bin√≥mica es una distribuci√≥n de probabilidad discreta que cuenta el n√∫mero de √©xitos en una secuencia de $$ùëõ$$ ensayos de Bernoulli independientes entre s√≠ con una probabilidad fija $$ùëù$$ de ocurrencia de √©xito entre los ensayos.

La distribuci√≥n binomial se utiliza con frecuencia para modelizar el n√∫mero de aciertos en una muestra de tama√±o $$ùëõ$$ extra√≠da con reemplazo de una poblaci√≥n de tama√±o N. 

Para cada experimento llamado **ensayo Bernoulli**, utilizamos una variable aleatoria que toma el valor 1 cuando se consigue un √©xito y el valor 0 en caso contrario. La variable aleatoria, suma de todas estas variables aleatorias, cuenta el n√∫mero de √©xitos y sigue una distribuci√≥n binomial. Es posible entonces obtener la probabilidad de $$k$$ √©xitos en una repetici√≥n de $$n$$ experimentos:

### Ejemplo Explicativo

Supongamos que lanzamos una moneda tres veces $$N = 3$$, y cada lanzamiento tiene una probabilidad de 0.7 de ser cara. Los posibles resultados (espacio muestral) y la distribuci√≥n de $$X$$ ser√≠an:

- **0 caras (X = 0):** $$(Cruz, Cruz, Cruz)$$
  - Probabilidad: $$(1 - 0.7)^3 = 0.027 $$
- **1 cara (X = 1):** $$(Cara, Cruz, Cruz), (Cruz, Cara, Cruz), (Cruz, Cruz, Cara)$$
  - Probabilidad: $$ \binom{3}{1} \times 0.7^1 \times 0.3^2 = 0.189 $$
- **2 caras (X = 2):** $$(Cara, Cara, Cruz), (Cruz, Cara, Cara), (Cara, Cruz, Cara)$$
  - Probabilidad: $$ \binom{3}{2} \times 0.7^2 \times 0.3^1 = 0.441$$
- **3 caras (X = 3):** $$(Cara, Cara, Cara)$$
  - Probabilidad: $$ 0.7^3 = 0.343 $$

### Ejemplo de C√≥digo en Python

Para trabajar con la variable aleatoria binomial en Python, una de las herramientas m√°s √∫tiles y comunes es el m√≥dulo `numpy`, espec√≠ficamente la funci√≥n `numpy.random.binomial`. Este m√≥dulo facilita la simulaci√≥n de ensayos binomiales, permitiendo generar datos que siguen una distribuci√≥n binomial. 

Este c√≥digo utiliza la funci√≥n `np.random.binomial` para simular 1000 ensayos binomiales donde se lanzan 10 monedas por ensayo con una probabilidad de 0.7 de obtener cara en cada lanzamiento. Luego, genera un histograma para visualizar la distribuci√≥n de los resultados.

```python
import numpy as np
import matplotlib.pyplot as plt

def simulate_binomial(n, p, trials):
    """Simula 'trials' ensayos binomiales de 'n' lanzamientos con probabilidad 'p'."""
    results = np.random.binomial(n, p, trials)
    return results

# Par√°metros de la simulaci√≥n
n = 10  # N√∫mero de lanzamientos
p = 0.7  # Probabilidad de cara
trials = 1000  # N√∫mero de simulaciones

# Realizar simulaciones
results = simulate_binomial(n, p, trials)

# Visualizar resultados
plt.hist(results, bins=np.arange(n+2)-0.5, edgecolor='black', density=True)
plt.title('Distribuci√≥n Binomial de Caras en 10 Lanzamientos')
plt.xlabel('N√∫mero de Caras')
plt.ylabel('Frecuencia Relativa')
plt.xticks(range(n+1))
plt.show()
```

![Visualizaci√≥n de la Convergencia](https://fer78docs.github.io/assets/images/distribucion_binomial.png)

### Funci√≥n de Masa de Probabilidad (PMF)

Para una distribuci√≥n binomial, la PMF **muestra la probabilidad de que un n√∫mero espec√≠fico de √©xitos ocurra en un n√∫mero fijo de ensayos**, cuando cada ensayo es independiente y tiene una probabilidad fija de √©xito.

### Ejemplo de Implementaci√≥n en Python

Podemos usar la biblioteca `scipy.stats` para calcular la PMF de una distribuci√≥n binomial.

```python
from scipy.stats import binom

# Par√°metros de la distribuci√≥n binomial
n = 10  # N√∫mero de ensayos
p = 0.5  # Probabilidad de √©xito en cada ensayo

# N√∫mero de √©xitos deseado
k = 5

# Calcular la PMF para k √©xitos
pmf = binom.pmf(k, n, p)

# Mostrar el resultado
print(f"PMF de {k} √©xitos en {n} ensayos con p = {p}: {pmf:.4f}")
```
Salida: 
```
PMF de 5 √©xitos en 10 ensayos con p = 0.5: 0.2461
```

### Interpretaci√≥n de la PMF Binomial

- La PMF te permite determinar la probabilidad de que un n√∫mero espec√≠fico de √©xitos ocurra dado un n√∫mero fijo de ensayos y una probabilidad conocida de √©xito.
- Es √∫til para modelar eventos donde hay un n√∫mero finito de pruebas independientes, cada una con dos posibles resultados, como √©xito/fracaso, cara/cruz, s√≠/no.

### Visualizaci√≥n de la PMF

La PMF tambi√©n puede ser visualizada utilizando gr√°ficos de barras, lo que ayuda a entender c√≥mo est√°n distribuidas las probabilidades de diferentes cantidades de √©xitos.

```python
import matplotlib.pyplot as plt

# Crear un rango de valores k desde 0 hasta n
k_values = range(n + 1)

# Calcular la PMF para cada valor de k
pmf_values = [binom.pmf(k, n, p) for k in k_values]

# Graficar la PMF
plt.bar(k_values, pmf_values)
plt.xlabel('N√∫mero de √©xitos (k)')
plt.ylabel('Probabilidad')
plt.title(f'PMF de una distribuci√≥n binomial (n={n}, p={p})')
plt.show()
```

![PMF binomial](https://fer78docs.github.io/assets/images/pmf_binoimial.png)

Este gr√°fico muestra c√≥mo las probabilidades se distribuyen para diferentes n√∫meros de √©xitos posibles en una distribuci√≥n binomial con par√°metros espec√≠ficos.

La **Funci√≥n de Distribuci√≥n Acumulativa (CDF)** para una distribuci√≥n binomial muestra la probabilidad acumulada de que una variable aleatoria tome un valor menor o igual a un valor espec√≠fico. 

La CDF se calcula sumando las probabilidades de la Funci√≥n de Masa de Probabilidad (PMF) para todos los valores desde 0 hasta $$k$$.

### Ejemplo de Implementaci√≥n en Python

Puedes utilizar `scipy.stats` para calcular y visualizar la CDF de una distribuci√≥n binomial.

```python
from scipy.stats import binom
import matplotlib.pyplot as plt

# Par√°metros de la distribuci√≥n binomial
n = 10  # N√∫mero de ensayos
p = 0.5  # Probabilidad de √©xito en cada ensayo

# Crear un rango de valores k desde 0 hasta n
k_values = range(n + 1)

# Calcular la CDF para cada valor de k
cdf_values = [binom.cdf(k, n, p) for k in k_values]

# Graficar la CDF
plt.step(k_values, cdf_values, where='mid', label=f'CDF de Binomial (n={n}, p={p})')
plt.xlabel('N√∫mero de √©xitos (k)')
plt.ylabel('Probabilidad acumulada')
plt.title('Funci√≥n de Distribuci√≥n Acumulativa (CDF) para una Distribuci√≥n Binomial')
plt.legend()
plt.grid(True)
plt.show()
```

![CDF binomial](https://fer78docs.github.io/assets/images/cdf_binomial.png)


### Interpretaci√≥n de la CDF Binomial

La CDF binomial es √∫til para comprender la probabilidad acumulada de obtener hasta cierto n√∫mero de √©xitos en un conjunto dado de ensayos. Permite responder preguntas como "¬øCu√°l es la probabilidad de obtener 5 o menos √©xitos?" en un experimento, y es crucial para el an√°lisis probabil√≠stico y la toma de decisiones en diversos campos como control de calidad, finanzas, investigaci√≥n biom√©dica, entre otros.

## Distribuci√≥n de Poisson

{: .note }
La distribuci√≥n de Poisson **se utiliza para describir la cantidad de veces que ocurre un determinado evento dentro de un intervalo de tiempo o espacio fijo**. Por ejemplo, la distribuci√≥n de Poisson se puede utilizar para describir la cantidad de autom√≥viles que pasan por una intersecci√≥n espec√≠fica entre las 4 p.m. y las 5 p.m. de un d√≠a determinado. Tambi√©n se puede utilizar para describir la cantidad de llamadas recibidas en una oficina entre las 13:00 y las 15:00 horas de un d√≠a determinado.

La distribuci√≥n de Poisson est√° definida por el par√°metro de tasa, simbolizado por la letra griega lambda, **Œª**.

Lambda representa el valor esperado (o el valor promedio) de la distribuci√≥n. Por ejemplo, si nuestro n√∫mero esperado de clientes entre la 1:00 p.m. y las 2:00 p.m. es 7, entonces establecer√≠amos el par√°metro para la distribuci√≥n de Poisson en 7. El PMF para la distribuci√≥n de Poisson(7) es el siguiente:

![Distribucion de Poisson](https://fer78docs.github.io/assets/images/poisson.png)

¬øC√≥mo se ve la PMF de la distribuci√≥n de Poisson con diferentes valores en el par√°metro lambda?

Utilice el control deslizante del subprograma para ver c√≥mo cambia la distribuci√≥n de la funci√≥n de masa de probabilidad a medida que lambda cambia entre 1 y 10. La altura de las barras representa la probabilidad de observar el n√∫mero a lo largo del eje x. Puede pasar el cursor sobre cualquier barra espec√≠fica y ver la probabilidad espec√≠fica de ese valor.

[Enlace al programa](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/poisson-slider/index.html)


### Calcular probabilidades de valores exactos utilizando la funci√≥n de masa de probabilidad

La distribuci√≥n de Poisson es una distribuci√≥n de probabilidad discreta, por lo que puede describirse mediante una funci√≥n de masa de probabilidad y una funci√≥n de distribuci√≥n acumulativa.

Podemos usar el m√©todo `poisson.pmf()` en la biblioteca `scipy.stats` para evaluar la probabilidad de observar un n√∫mero espec√≠fico dado el par√°metro (valor esperado) de una distribuci√≥n. 

Por ejemplo, supongamos que esperamos que llueva 10 veces en los pr√≥ximos 30 d√≠as. El n√∫mero de veces que llueve en los pr√≥ximos 30 d√≠as tiene ‚Äúdistribuci√≥n de Poisson‚Äù con lambda = 10. Podemos calcular la probabilidad de que llueva exactamente 6 veces de la siguiente manera:

```python
import scipy.stats as stats
# expected value = 10, probability of observing 6
stats.poisson.pmf(6, 10)
```
Output
```
0.06305545800345125
```
Al igual que las funciones de masa de probabilidad anteriores de variables aleatorias discretas, las probabilidades individuales se pueden sumar para encontrar la probabilidad de observar un valor en un rango.

Por ejemplo, si esperamos que llueva 10 veces en los pr√≥ximos 30 d√≠as, el n√∫mero de veces que llueve en los pr√≥ximos 30 d√≠as tiene una ‚Äúdistribuci√≥n de Poisson‚Äù con lambda = 10. Podemos calcular la probabilidad de que llueva entre 12 y 14 veces. como sigue:

```python
import scipy.stats as stats
# expected value = 10, probability of observing 12-14
stats.poisson.pmf(12, 10) 
    + stats.poisson.pmf(13, 10) 
    + stats.poisson.pmf(14, 10)
```
Output
```
0.21976538076223123
```

### Calcular probabilidades de un rango usando la funci√≥n de densidad acumulativa

Podemos usar el m√©todo `poisson.cdf()` en la biblioteca `scipy.stats` para evaluar la probabilidad de observar un n√∫mero espec√≠fico o menos dado el valor esperado de una distribuci√≥n. Por ejemplo, si quisi√©ramos calcular la probabilidad de observar 6 o menos eventos de lluvia en los pr√≥ximos 30 d√≠as cuando esper√°bamos 10, podr√≠amos hacer lo siguiente:

```python
import scipy.stats as stats
# expected value = 10, probability of observing 6 or less
stats.poisson.cdf(6, 10)
```
Output
```
0.130141420882483
```

Esto significa que hay aproximadamente un 13% de posibilidades de que haya 6 lluvias o menos en el mes en cuesti√≥n.

Tambi√©n podemos utilizar este m√©todo para evaluar la probabilidad de observar un n√∫mero espec√≠fico o m√°s dado el valor esperado de la distribuci√≥n. Por ejemplo, si quisi√©ramos calcular la probabilidad de observar 12 o m√°s eventos de lluvia en los pr√≥ximos 30 d√≠as cuando esper√°bamos 10, podr√≠amos hacer lo siguiente:

```python
import scipy.stats as stats
# expected value = 10, probability of observing 12 or more
1 - stats.poisson.cdf(11, 10)
```
Output
```
0.30322385369689386
```

Esto significa que hay aproximadamente un 30% de posibilidades de que haya 12 o m√°s lluvias en el mes en cuesti√≥n.

Tenga en cuenta que utilizamos 11 en la declaraci√≥n anterior a pesar de que 12 era el valor dado en el mensaje. Quer√≠amos calcular la probabilidad de observar 12 o m√°s lluvias, lo que incluye 12. `stats.poisson.cdf(11, 10)` Eval√∫a la probabilidad de observar 11 o menos lluvias, por lo que `1 - stats.poisson.cdf(11, 10)` ser√≠a igual a la probabilidad de observar 12 o m√°s lluvias.

Sumar probabilidades individuales en un amplio rango puede resultar engorroso. A menudo es m√°s f√°cil calcular la probabilidad de un rango utilizando la funci√≥n de densidad acumulada en lugar de la funci√≥n de masa de probabilidad. Podemos hacer esto tomando la diferencia entre la CDF del punto final m√°s grande y la CDF de uno menos que el punto final m√°s peque√±o del rango.

Por ejemplo, aunque todav√≠a esperamos 10 lluvias en los pr√≥ximos 30 d√≠as, podr√≠amos usar el siguiente c√≥digo para calcular la probabilidad de observar entre 12 y 18 eventos de lluvia:

```python
import scipy.stats as stats
# expected value = 10, probability of observing between 12 and 18
stats.poisson.cdf(18, 10) - stats.poisson.cdf(11, 10)
```
Output
```
0.29603734909303947
```

### Expectativa de la distribuci√≥n de Poisson

Anteriormente mencionamos que el par√°metro lambda (Œª) es el valor esperado (o valor promedio) de la distribuci√≥n de Poisson. Pero ¬øqu√© significa esto?

Pongamos esto en contexto: digamos que somos vendedores, y despu√©s de muchas semanas de trabajo, calculamos que nuestro promedio es de 10 ventas por semana. Si tomamos este valor como nuestro valor esperado de una distribuci√≥n de Poisson, la funci√≥n de masa de probabilidad tendr√° el siguiente aspecto:

![Distribucion de Poisson](https://fer78docs.github.io/assets/images/funcion_de_pm.png)

La barra m√°s alta representa el valor con mayor probabilidad de ocurrir. En este caso, la barra m√°s alta est√° en 10. Sin embargo, esto no significa que realizaremos 10 ventas. Significa que, en promedio, en todas las semanas, esperamos que nuestro promedio sea de aproximadamente 10 ventas por semana.

Miremos esto de otra manera. Tomemos una muestra de 1000 valores aleatorios de la distribuci√≥n de Poisson con el valor esperado de 10. Podemos usar el m√©todo `poisson.rvs()` en la biblioteca `scipy.stats` para generar valores aleatorios:

```python
import scipy.stats as stats

# generate random variable
# stats.poisson.rvs(lambda, size = num_values)
rvs = stats.poisson.rvs(10, size = 1000)
```
El histograma de este muestreo se parece al siguiente:

![Distribucion de Poisson](https://fer78docs.github.io/assets/images/poisson_rvs.png)

Podemos ver observaciones tan bajas como 2 pero tan altas como 20. Las barras m√°s altas est√°n en 9 y 10. Si tom√°ramos el promedio de las 1000 muestras aleatorias, obtendr√≠amos:

```python
print(rvs.mean())
```
Output:
```
10.009
```

Este valor est√° muy cerca de 10, lo que confirma que sobre las 1000 observaciones, el valor esperado (o promedio) es 10.

Cuando hablamos del valor esperado, nos referimos al promedio de muchas observaciones. Esto se relaciona con la Ley de los Grandes N√∫meros: cuantas m√°s muestras tengamos, es m√°s probable que las muestras se parezcan a la poblaci√≥n real y la media de las muestras se acerque al valor esperado. Entonces, aunque el vendedor pueda realizar 3 ventas una semana, puede realizar 16 la siguiente y 11 la semana siguiente. A largo plazo, despu√©s de muchas semanas, el valor esperado (o promedio) seguir√≠a siendo 10.

## Propagaci√≥n de la distribuci√≥n de Poisson

Las distribuciones de probabilidad tambi√©n tienen varianzas calculables. Las varianzas son una forma de medir la dispersi√≥n de valores y probabilidades en la distribuci√≥n. Para la distribuci√≥n de Poisson, la varianza es simplemente el valor de lambda (Œª), lo que significa que el valor esperado y la varianza son equivalentes en las distribuciones de Poisson.

Sabemos que la distribuci√≥n de Poisson tiene una variable aleatoria discreta y debe ser mayor que 0 (piense, un vendedor no puede tener menos de 0 ventas, una tienda no puede tener menos de 0 clientes), por lo que a medida que aumenta el valor esperado, el n√∫mero de posibles Los valores que la distribuci√≥n puede asumir tambi√©n aumentar√≠an.

El primer gr√°fico a continuaci√≥n muestra una distribuci√≥n de Poisson con lambda igual a tres, y el segundo gr√°fico muestra una distribuci√≥n de Poisson con lambda igual a quince. Observe que en el segundo gr√°fico, la dispersi√≥n de la distribuci√≥n aumenta. Adem√°s, tenga en cuenta que la altura de las barras en la segunda barra disminuye ya que hay m√°s valores posibles en la distribuci√≥n.

![Distribucion de Poisson 1 ](https://fer78docs.github.io/assets/images/distribucion_poisson_1.png)

![Distribucion de Poisson 2 ](https://fer78docs.github.io/assets/images/distribucion_poisson_2.png)

Como podemos ver, a medida que aumenta el par√°metro lambda, tambi√©n aumenta la varianza (o dispersi√≥n) de los valores posibles.

Podemos calcular la varianza de una muestra usando el m√©todo `numpy.var()`:

```python
import scipy.stats as stats
import numpy as np

rand_vars = stats.poisson.rvs(4, size = 1000)
print(np.var(rand_vars))
```
Output:
```
3.864559
```

Debido a que esto se calcula a partir de una muestra, es posible que la varianza no sea EXACTAMENTE igual a lambda. Sin embargo, esperamos que sea relativamente cercano cuando el tama√±o de la muestra es grande, como en este ejemplo.

Otra forma de ver el aumento de los valores posibles es tomar el rango de una muestra (los valores m√≠nimo y m√°ximo de un conjunto). El siguiente c√≥digo extraer√° 1000 variables aleatorias de la distribuci√≥n de Poisson con lambda = 4 y luego imprimir√° los valores m√≠nimo y m√°ximo observados usando las funciones `.min()` y `.max()` de Python:

```python
import scipy.stats as stats

rand_vars = stats.poisson.rvs(4, size = 1000)

print(min(rand_vars), max(rand_vars))
```
Output:
```
0 12
```

Si aumentamos el valor de lambda a 10, veamos c√≥mo cambian los valores m√≠nimo y m√°ximo:

```python
import scipy.stats as stats

rand_vars = stats.poisson.rvs(10, size = 1000)

print(min(rand_vars), max(rand_vars))
```
Output:
```
1 22
```

Estos valores est√°n m√°s distribuidos, lo que indica una variaci√≥n mayor.

## Distribucion Geometrica

{: .note}
La distribuci√≥n geom√©trica es una distribuci√≥n de probabilidad discreta que **describe el n√∫mero de intentos necesarios para lograr el primer √©xito en una serie de ensayos independientes, cada uno con una probabilidad constante de √©xito.** Es una distribuci√≥n discreta porque el n√∫mero de intentos es un valor entero.

### Ensayos Independientes de Bernoulli

{: .highlight}
**Un ensayo independiente implica que el resultado de un ensayo que no influye ni puede ser influenciado por los resultados de otros ensayos.** En el contexto de lanzar una moneda, significa que el resultado de un lanzamiento no afecta el resultado del siguiente.

#### Ejemplo con Dos Lanzamientos
Si lanzamos una moneda dos veces con una probabilidad de 0.7 de sacar cara en cada lanzamiento, la probabilidad de obtener dos caras seguidas ser√≠a $$0.7 \times 0.7 = 0.49$$.

#### Explicaci√≥n y Ejemplo
Supongamos que lanzamos una moneda hasta que salga cara por primera vez, y queremos saber cu√°ntos lanzamientos se necesitan.

```python
def geometric_trial(p=0.7):
    """Simula una variable aleatoria geom√©trica.
    Args:
        p (float): Probabilidad de sacar cara. 
    Returns:
        int: N√∫mero de lanzamientos hasta el primer √©xito.
    """
    count = 1
    while bernoulli_trial(p) == 0:
        count += 1
    return count
```

Esta funci√≥n utiliza la funci√≥n de Bernoulli definida anteriormente para simular lanzamientos sucesivos hasta que el primer lanzamiento resulte en √©xito (cara).

### Funci√≥n de Masa de Probabilidad (PMF) de la Variable Geom√©trica

 La PMF de una variable aleatoria geom√©trica da la probabilidad de que se necesite un n√∫mero espec√≠fico de ensayos para alcanzar el primer √©xito. 

#### Ejemplo

- $$P(X=1) = 0.7$$ (sacar cara en el primer lanzamiento).
- $$P(X=2) = 0.3 \times 0.7$$ (sacar cruz en el primero y cara en el segundo).
- $$P(X=n) = 0.3^{(n-1)} \times 0.7$$ (sacar cruz en los primeros $$n-1$$ lanzamientos y cara en el $$n$$-√©simo).

### Prueba de normalizaci√≥n de variable aleatoria geom√©trica opcional

Este segmento explica la variable aleatoria geom√©trica en detalle, destacando su importancia en el contexto de ensayos independientes de Bernoulli, como el lanzamiento repetido de una moneda hasta obtener el primer √©xito (cara). Adem√°s, se aborda la propiedad de normalizaci√≥n de la distribuci√≥n geom√©trica y se anticipa una demostraci√≥n pr√°ctica utilizando Python. Vamos a desglosar y ampliar estos conceptos y ejemplos en espa√±ol.

### Variable Aleatoria Geom√©trica

{: .highlight}
La variable aleatoria geom√©trica mide el n√∫mero de intentos necesarios hasta obtener el primer √©xito en una serie de ensayos independientes de Bernoulli. Por ejemplo, en el lanzamiento de una moneda, esta variable contar√≠a cu√°ntos lanzamientos se realizan hasta que aparece la primera cara, asumiendo que cada lanzamiento es independiente del anterior.

#### Propiedades de la Variable Geom√©trica

- **Espacio Muestral Infinito pero Contable:** La variable geom√©trica puede tomar una cantidad infinita de valores (1, 2, 3, ...), ya que te√≥ricamente podr√≠an requerirse infinitos lanzamientos para obtener una cara. Sin embargo, estos valores son contables, lo que clasifica a la variable como discreta.
- **Distribuci√≥n de Probabilidad:** La probabilidad de que la variable aleatoria geom√©trica tome un valor espec√≠fico $$k$$ es una funci√≥n de la probabilidad de √©xito $$p$$ y la probabilidad de fracaso $$q = 1-p$$. donde $$k$$ es el n√∫mero de lanzamientos realizados hasta obtener el primer √©xito.

$$P(X = k) = q^{k-1} \cdot p$$

### Aplicaci√≥n Pr√°ctica en Python

Este c√≥digo simula 1000 ensayos de una variable aleatoria geom√©trica y utiliza un histograma para visualizar cu√°ntos lanzamientos se necesitan t√≠picamente para obtener el primer √©xito. Esta visualizaci√≥n ayuda a comprender la naturaleza de la distribuci√≥n geom√©trica y la efectividad de los ensayos de Bernoulli independientes.

```python
import numpy as np
import matplotlib.pyplot as plt

def geometric_trial(p=0.7):
    count = 0
    while np.random.rand() >= p:
        count += 1
    return count + 1

# Simulaci√≥n de 1000 ensayos geom√©tricos
results = [geometric_trial(p=0.7) for _ in range(1000)]

# Visualizaci√≥n de la distribuci√≥n
plt.hist(results, bins=max(results), edgecolor='black')
plt.title('Distribuci√≥n de la Variable Aleatoria Geom√©trica')
plt.xlabel('N√∫mero de Lanzamientos hasta el Primer √âxito')
plt.ylabel('Frecuencia')
plt.show()
```
![Visualizaci√≥n de la Convergencia](https://fer78docs.github.io/assets/images/variable_aleatoria_geom√©trica.png)



